#!/usr/bin/env python
# coding: utf-8
"""
ABSA Randomâ€‘Split Recommender â€” Recencyâ€‘Weighted Âµâˆ¥ÏƒÂ² Pooling
------------------------------------------------------------
í”¼ë“œë°± ë°˜ì˜ ì‚¬í•­
1. **ë‹¨ìˆœ í‰ê·  â†’ (ìµœê·¼ì„± ê°€ì¤‘) í‰ê·  + ë¶„ì‚°(Âµ âˆ¥ ÏƒÂ²) ë²¡í„°**
2. **TFâ€‘IDF ì œê±°** â†’ ì‹œê°„ ê°ì‡ (halfâ€‘life)ë§Œ ì‚¬ìš©
3. ë‚˜ë¨¸ì§€ íŒŒì´í”„ë¼ì¸(ëœë¤ ìŠ¤í”Œë¦¿, user2seen í•„í„°, í‰ê°€ ë£¨í”„) ìœ ì§€
"""
import json
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

import numpy as np
from sklearn.metrics import precision_score  # (ë‚¨ì•„ ìˆì§€ë§Œ ì‚¬ìš© ì•ˆ í•¨)
from sklearn.preprocessing import normalize
from tqdm import tqdm

# ---------------------------------------------------------------------------
# ì„¤ì •
# ---------------------------------------------------------------------------
TRAIN_FILE = "dataset/train_80_random_with_date.json"
TEST_FILE = "dataset/test_20_random_with_date.json"
TOP_N_LIST = [5, 10]
HALF_LIFE_DAYS = 180  # ìµœê·¼ì„± halfâ€‘life (â‰ˆ 6 ê°œì›”)

# ---------------------------------------------------------------------------
# ìœ í‹¸
# ---------------------------------------------------------------------------


def load_reviews(path: str):
    data = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)

            # sentiment ê°ì²´ì—ì„œ ë²¡í„° ìƒì„± (5 aspects x 3 sentiments = 15 dimension)
            if "sentiment" in obj and "sentiment_vector" not in obj:
                sent_vector = []
                aspects = ["food", "service", "price", "ambience", "location"]
                sentiments = ["Negative", "Neutral", "Positive"]

                for aspect in aspects:
                    if aspect in obj["sentiment"]:
                        for sentiment in sentiments:
                            sent_vector.append(
                                obj["sentiment"][aspect]["scores"][sentiment]
                            )

                obj["sentiment_vector"] = sent_vector

            # ê¸°ì¡´ ë²¡í„° í•„ë“œ ì‚¬ìš©
            obj["vector"] = np.array(obj["sentiment_vector"], dtype=float)

            # ë‚ ì§œ íŒŒì‹± (ISO 8601 or "YYYY-MM-DD")
            obj["date"] = datetime.fromisoformat(obj["date"])
            data.append(obj)
    return data


def recency_weight(days: int) -> float:
    """ì§€ìˆ˜ ê°ì‡  ê°€ì¤‘ì¹˜ (halfâ€‘life = HALF_LIFE_DAYS)."""
    if days < 0:
        days = 0
    return np.exp(-np.log(2) * days / HALF_LIFE_DAYS)


def agg_mu_sigma(rows: List[Tuple[np.ndarray, datetime]]) -> np.ndarray:
    """ìµœê·¼ì„± ê°€ì¤‘ í‰ê· (Âµ) + ë¶„ì‚°(ÏƒÂ²) concat ë°˜í™˜."""
    if not rows:
        return np.zeros(10)  # placeholder; ì‹¤ì œ dimì€ ë™ì ìœ¼ë¡œ ì²˜ë¦¬ ì•„ë˜ ì°¸ê³ 

    now = datetime.now()
    vecs, ws = [], []
    for v, ts in rows:
        w = recency_weight((now - ts).days)
        vecs.append(v)
        ws.append(w)
    V = np.vstack(vecs)  # (n, d)
    W = np.array(ws).reshape(-1, 1)  # (n, 1)

    mu = (W * V).sum(axis=0) / W.sum()
    var = (W * (V - mu) ** 2).sum(axis=0) / W.sum()

    return np.concatenate([mu, var])


# ---------------------------------------------------------------------------
# nDCG ê³„ì‚°
# ---------------------------------------------------------------------------


def compute_ndcg(pred: List[str], actual: set, k: int) -> float:
    if not actual:
        return 0.0
    relevance = np.array([1.0 if bid in actual else 0.0 for bid in pred[:k]])
    dcg = np.sum(relevance / np.log2(np.arange(2, len(relevance) + 2)))
    ideal = np.ones(min(len(actual), k))
    idcg = np.sum(ideal / np.log2(np.arange(2, len(ideal) + 2)))
    return dcg / idcg if idcg else 0.0


# ---------------------------------------------------------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ---------------------------------------------------------------------------


def main():
    print("ğŸ”„ ëœë¤ ìŠ¤í”Œë¦¿ ë°ì´í„°ë¡œ ì¶”ì²œ ëª¨ë¸ í‰ê°€ ì‹œì‘")

    # ë°ì´í„° ë¡œë“œ
    train_reviews = load_reviews(TRAIN_FILE)
    test_reviews = load_reviews(TEST_FILE)
    print(
        f"ğŸ“š í›ˆë ¨ ë°ì´í„°: {len(train_reviews)}ê°œ,  ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_reviews)}ê°œ"
    )

    # --------------------------------------------------
    # ì‚¬ìš©ìÂ·ë¹„ì¦ˆë‹ˆìŠ¤ ë²¡í„° ì§‘ê³„ (Âµâˆ¥ÏƒÂ² pooling)
    # --------------------------------------------------
    user_rows = defaultdict(list)  # uid â†’ [(vec, date), ...]
    biz_rows = defaultdict(list)  # bid â†’ [(vec, date), ...]

    for r in train_reviews:
        user_rows[r["user_id"]].append((r["vector"], r["date"]))
        biz_rows[r["business_id"]].append((r["vector"], r["date"]))

    user_embed = {u: agg_mu_sigma(rows) for u, rows in user_rows.items()}
    biz_embed = {b: agg_mu_sigma(rows) for b, rows in biz_rows.items()}

    print(f"ğŸ‘¤ ì‚¬ìš©ì ì„ë² ë”©: {len(user_embed)}ëª…,  ğŸª ì‹ë‹¹ ì„ë² ë”©: {len(biz_embed)}ê°œ")

    # ì°¨ì› í™•ì¸ & 0â€‘ë²¡í„° ì œê±°
    user_embed = {u: v for u, v in user_embed.items() if np.linalg.norm(v) > 0}
    biz_embed = {b: v for b, v in biz_embed.items() if np.linalg.norm(v) > 0}

    user_ids = list(user_embed.keys())
    biz_ids = list(biz_embed.keys())

    user_matrix = normalize(np.stack([user_embed[u] for u in user_ids]))
    biz_matrix = normalize(np.stack([biz_embed[b] for b in biz_ids]))

    scores = user_matrix @ biz_matrix.T
    print("âœ… ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° ì™„ë£Œ")

    # --------------------------------------------------
    # ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    # --------------------------------------------------
    user2seen = defaultdict(set)
    for r in train_reviews:
        user2seen[r["user_id"]].add(r["business_id"])

    max_k = max(TOP_N_LIST)
    recommendations = {}

    print("ğŸ” ì¶”ì²œ ëª©ë¡ ìƒì„± ì¤‘â€¦")
    for i, uid in enumerate(tqdm(user_ids)):
        ranked_idx = np.argsort(scores[i])[::-1]
        recs = []
        for j in ranked_idx:
            bid = biz_ids[j]
            if bid not in user2seen[uid]:  # ì¬ë°©ë¬¸ ì œì™¸ ë¡œì§ ìœ ì§€
                recs.append(bid)
            if len(recs) == max_k:
                break
        recommendations[uid] = recs

    # --------------------------------------------------
    # í‰ê°€
    # --------------------------------------------------
    ground_truth = defaultdict(set)
    for r in test_reviews:
        ground_truth[r["user_id"]].add(r["business_id"])

    common_users = set(recommendations) & set(ground_truth)
    print(f"ğŸ“Œ í‰ê°€ ëŒ€ìƒ ìœ ì €: {len(common_users)}ëª…")

    for k in TOP_N_LIST:
        precisions, recalls, ndcgs = [], [], []
        for uid in common_users:
            pred = recommendations[uid][:k]
            actual = ground_truth[uid]
            hit = len(set(pred) & actual)
            precisions.append(hit / k)
            recalls.append(hit / len(actual) if actual else 0)
            ndcgs.append(compute_ndcg(pred, actual, k))
        print(f"\n===== K = {k} í‰ê°€ ì§€í‘œ =====")
        print(f"ğŸ¯ Precision@{k}: {np.mean(precisions):.4f}")
        print(f"ğŸ” Recall@{k}:    {np.mean(recalls):.4f}")
        print(f"ğŸ“Š nDCG@{k}:      {np.mean(ndcgs):.4f}")


if __name__ == "__main__":
    main()
