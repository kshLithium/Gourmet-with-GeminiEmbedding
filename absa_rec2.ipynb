{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40909e3a",
   "metadata": {},
   "source": [
    "### ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5a8f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìœ ì € ìˆ˜: 28506\n",
      "ðŸ“¦ í›ˆë ¨ ìœ ì € ìˆ˜: 22805, ë¦¬ë·° ìˆ˜: 327083\n",
      "ðŸ§ª í…ŒìŠ¤íŠ¸ ìœ ì € ìˆ˜: 5701, ë¦¬ë·° ìˆ˜: 79594\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ì„¤ì •\n",
    "INPUT_FILE = \"absa_ate_results.json\"\n",
    "TRAIN_FILE = \"absa_train.json\"\n",
    "TEST_FILE = \"absa_test.json\"\n",
    "TEST_RATIO = 0.2\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load and Group by user\n",
    "# --------------------------\n",
    "user_reviews = defaultdict(list)\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        if obj.get(\"aspects\"):  # aspect ì—†ëŠ” ë¦¬ë·° ì œì™¸\n",
    "            user_reviews[obj[\"user_id\"]].append(obj)\n",
    "\n",
    "print(f\"âœ… ìœ ì € ìˆ˜: {len(user_reviews)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Split by user\n",
    "# --------------------------\n",
    "user_ids = list(user_reviews.keys())\n",
    "random.shuffle(user_ids)\n",
    "\n",
    "n_test_users = int(len(user_ids) * TEST_RATIO)\n",
    "test_users = set(user_ids[:n_test_users])\n",
    "train_users = set(user_ids[n_test_users:])\n",
    "\n",
    "train_data, test_data = [], []\n",
    "\n",
    "for uid in train_users:\n",
    "    train_data.extend(user_reviews[uid])\n",
    "\n",
    "for uid in test_users:\n",
    "    test_data.extend(user_reviews[uid])\n",
    "\n",
    "# --------------------------\n",
    "# 3. Save to files\n",
    "# --------------------------\n",
    "with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in train_data:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(TEST_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in test_data:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"ðŸ“¦ í›ˆë ¨ ìœ ì € ìˆ˜: {len(train_users)}, ë¦¬ë·° ìˆ˜: {len(train_data)}\")\n",
    "print(f\"ðŸ§ª í…ŒìŠ¤íŠ¸ ìœ ì € ìˆ˜: {len(test_users)}, ë¦¬ë·° ìˆ˜: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316e360",
   "metadata": {},
   "source": [
    "####word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11da7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ ì¶”ì²œ ê³„ì‚° ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21211/21211 [00:32<00:00, 649.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¶”ì²œ ì €ìž¥ ì™„ë£Œ: recommendations_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ðŸ”§ íŒŒì¼ ì„¤ì • (filtered ë²¡í„° ì‚¬ìš©)\n",
    "USER_VEC_FILE = \"user_vector_filtered.json\"\n",
    "BIZ_VEC_FILE = \"business_vector_filtered.json\"\n",
    "TRAIN_FILE = \"absa_train.json\"\n",
    "RECOMMEND_FILE = \"recommendations_filtered.json\"\n",
    "TOP_N = 10\n",
    "\n",
    "# ðŸ“¦ ë²¡í„° ë¡œë”©\n",
    "with open(USER_VEC_FILE, encoding=\"utf-8\") as f:\n",
    "    user_embed = json.load(f)\n",
    "with open(BIZ_VEC_FILE, encoding=\"utf-8\") as f:\n",
    "    biz_embed = json.load(f)\n",
    "\n",
    "# ðŸ‘€ í›ˆë ¨ ë°ì´í„° ê¸°ë°˜ ì‚¬ìš©ìž ë°©ë¬¸ ì‹ë‹¹ ê¸°ë¡\n",
    "user_seen = defaultdict(set)\n",
    "with open(TRAIN_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        user_seen[obj[\"user_id\"]].add(obj[\"business_id\"])\n",
    "\n",
    "# ðŸ§® ì •ê·œí™” ë° í–‰ë ¬ êµ¬ì„±\n",
    "uids = list(user_embed.keys())\n",
    "bids = list(biz_embed.keys())\n",
    "u_matrix = normalize(np.array([user_embed[uid] for uid in uids]), axis=1)\n",
    "b_matrix = normalize(np.array([biz_embed[bid] for bid in bids]), axis=1)\n",
    "\n",
    "# ðŸ§  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = dot product (ì •ê·œí™” ë˜ì–´ ìžˆìŒ)\n",
    "sim = np.dot(u_matrix, b_matrix.T)\n",
    "\n",
    "# ðŸŽ¯ ì¶”ì²œ ê³„ì‚°\n",
    "recommendations = {}\n",
    "for i, uid in enumerate(tqdm(uids, desc=\"ðŸ“¡ ì¶”ì²œ ê³„ì‚° ì¤‘\")):\n",
    "    seen = user_seen[uid]\n",
    "    ranked_idx = np.argsort(sim[i])[::-1]\n",
    "    top_biz = [bids[j] for j in ranked_idx if bids[j] not in seen][:TOP_N]\n",
    "    recommendations[uid] = top_biz\n",
    "\n",
    "# ðŸ’¾ ê²°ê³¼ ì €ìž¥\n",
    "with open(RECOMMEND_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(recommendations, f, indent=2)\n",
    "\n",
    "print(\"âœ… ì¶”ì²œ ì €ìž¥ ì™„ë£Œ:\", RECOMMEND_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28afc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ í‰ê°€ ëŒ€ìƒ ìœ ì € ìˆ˜: 4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Precision@K ê³„ì‚° ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4214/4214 [00:00<00:00, 63903.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Precision@K ê²°ê³¼\n",
      "ðŸ”¹ Top-1 â†’ Precision: 0.0994, Recall: 0.0111\n",
      "ðŸ”¹ Top-5 â†’ Precision: 0.0570, Recall: 0.0299\n",
      "ðŸ”¹ Top-10 â†’ Precision: 0.0444, Recall: 0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# íŒŒì¼ ì„¤ì •\n",
    "RECOMMEND_FILE = \"recommendations_filtered.json\"\n",
    "EVAL_FILE = \"absa_test.json\"\n",
    "TOP_K = [1, 5, 10]  # ì›í•˜ëŠ” í‰ê°€ ê¸°ì¤€\n",
    "\n",
    "# ì¶”ì²œ ê²°ê³¼ ë¡œë”©\n",
    "with open(RECOMMEND_FILE, encoding=\"utf-8\") as f:\n",
    "    recommendations = json.load(f)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ì…‹ (ì‹¤ì œ ë°©ë¬¸í•œ ì‹ë‹¹ ground truth)\n",
    "ground_truth = defaultdict(set)\n",
    "with open(EVAL_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        ground_truth[obj[\"user_id\"]].add(obj[\"business_id\"])\n",
    "\n",
    "# í‰ê°€ ëŒ€ìƒ ì‚¬ìš©ìž: ì¶”ì²œê³¼ ì‹¤ì œ ëª¨ë‘ ìžˆëŠ” ì‚¬ìš©ìžë§Œ\n",
    "common_users = set(recommendations.keys()) & set(ground_truth.keys())\n",
    "print(f\"ðŸ“Œ í‰ê°€ ëŒ€ìƒ ìœ ì € ìˆ˜: {len(common_users)}\")\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ì´ˆê¸°í™”\n",
    "metrics = {k: {\"precision\": 0, \"recall\": 0, \"f1\": 0} for k in TOP_K}\n",
    "\n",
    "for uid in tqdm(common_users, desc=\"ðŸŽ¯ Precision@K ê³„ì‚° ì¤‘\"):\n",
    "    pred = recommendations[uid]\n",
    "    true = ground_truth[uid]\n",
    "\n",
    "    for k in TOP_K:\n",
    "        topk_pred = set(pred[:k])\n",
    "        hits = topk_pred & true\n",
    "\n",
    "        precision = len(hits) / k\n",
    "        recall = len(hits) / len(true) if true else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "        metrics[k][\"precision\"] += precision\n",
    "        metrics[k][\"recall\"] += recall\n",
    "        metrics[k][\"f1\"] += f1\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "n = len(common_users)\n",
    "print(\"\\nðŸ“Š Precision@K ê²°ê³¼\")\n",
    "for k in TOP_K:\n",
    "    p = metrics[k][\"precision\"] / n\n",
    "    r = metrics[k][\"recall\"] / n\n",
    "    f = metrics[k][\"f1\"] / n\n",
    "    print(f\"ðŸ”¹ Top-{k} â†’ Precision: {p:.4f}, Recall: {r:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21111fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
