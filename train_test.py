#!/usr/bin/env python
# coding: utf-8

# #### ê°ì„±ë¶„ì„ ê²°ê³¼ì— ìœ ì €ID, ì‹ë‹¹ID ì¶”ê°€

# In[3]:


import json
from tqdm import tqdm

# íŒŒì¼ ê²½ë¡œ
full_data_file = "dataset/review_5up.json"
target_file = "dataset/review_5up_5aspect_3sentiment.jsonl"
output_file = "dataset/review_5up_5aspect_3sentiment_with_ids.jsonl"
# ì „ì²´ ë¦¬ë·° ë°ì´í„°ì—ì„œ review_id â†’ (user_id, business_id) ë§µ ìƒì„±
id_map = {}

with open(full_data_file, "r", encoding="utf-8") as f:
    for line in f:
        obj = json.loads(line)
        rid = obj.get("review_id")
        if rid:
            id_map[rid] = {
                "user_id": obj.get("user_id"),
                "business_id": obj.get("business_id"),
            }

print(f" ì „ì²´ ë¦¬ë·°ì—ì„œ ë§¤í•‘ëœ review_id ìˆ˜: {len(id_map)}")

# 2. ëŒ€ìƒ íŒŒì¼ ì½ê³  user_id, business_id ì¶”ê°€
updated = []
missing = []

with open(target_file, "r", encoding="utf-8") as f:
    for line in tqdm(f, desc="ğŸ”„ ID ì¶”ê°€ ì¤‘"):
        obj = json.loads(line)
        rid = obj.get("review_id")
        if rid in id_map:
            obj.update(id_map[rid])  # user_id, business_id ì¶”ê°€
            updated.append(obj)
        else:
            missing.append(obj)  # ë§¤ì¹­ ì•ˆ ë˜ëŠ” ê²½ìš° ë”°ë¡œ ë³´ê´€

# 3. ê²°ê³¼ ì €ì¥
with open(output_file, "w", encoding="utf-8") as f:
    for obj in updated:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

print(f" ì €ì¥ ì™„ë£Œ: ì´ {len(updated)}ê±´ â†’ {output_file}")
if missing:
    print(f" ë§¤ì¹­ ì•ˆëœ ë¦¬ë·° ìˆ˜: {len(missing)}")


# #### ì¶”ì²œí˜•ì‹ì— ë§ê²Œ ë³€í™˜

# In[11]:


import json

input_file = "dataset/review_5up_5aspect_3sentiment_with_ids.jsonl"
output_file = "dataset/review_5up_5aspect_3sentiment_vectorized_clean.json"


def sentiment_to_vector(sentiment_dict):
    aspects = ["food", "service", "price", "ambience", "location"]
    polarities = ["Negative", "Neutral", "Positive"]
    vector = []
    for asp in aspects:
        scores = sentiment_dict.get(asp, {}).get("scores", {})
        for pol in polarities:
            vector.append(scores.get(pol, 0.0))
    return vector


with open(input_file, "r", encoding="utf-8") as fin, open(
    output_file, "w", encoding="utf-8"
) as fout:

    for line in fin:
        obj = json.loads(line)

        # ë²¡í„° ìƒì„±
        sentiment_vec = sentiment_to_vector(obj.get("sentiment", {}))

        # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ ë° ì¬êµ¬ì„±
        cleaned = {
            "review_id": obj.get("review_id"),
            "user_id": obj.get("user_id"),
            "business_id": obj.get("business_id"),
            "sentiment_vector": sentiment_vec,
        }

        fout.write(json.dumps(cleaned, ensure_ascii=False) + "\n")

print(f"âœ… ì™„ë£Œ: textì™€ sentiment ì œê±° í›„ ì €ì¥ â†’ {output_file}")


# #### ìœ ì €ë³„ í‰ê·  ë¦¬ë·°ìˆ˜ í™•ì¸

# In[12]:


import json
from collections import defaultdict

# íŒŒì¼ ê²½ë¡œ
INPUT_FILE = "dataset/review_5up_5aspect_3sentiment_vectorized_clean.json"

# ìœ ì €ë³„ ë¦¬ë·° ìˆ˜ ì €ì¥
user_review_counts = defaultdict(int)

# ë°ì´í„° ë¡œë”©
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    for line in f:
        review = json.loads(line)
        user_id = review["user_id"]
        user_review_counts[user_id] += 1

# í†µê³„ ê³„ì‚°
total_users = len(user_review_counts)
total_reviews = sum(user_review_counts.values())
avg_reviews_per_user = total_reviews / total_users

print(f"ğŸ“Š ì´ ìœ ì € ìˆ˜: {total_users}")
print(f"ğŸ“ ì´ ë¦¬ë·° ìˆ˜: {total_reviews}")
print(f"ğŸ“ˆ ìœ ì €ë‹¹ í‰ê·  ë¦¬ë·° ìˆ˜: {avg_reviews_per_user:.2f}")


# #### í›ˆë ¨ 80% / í‰ê°€20% ë¡œ ë¶„í• 

# In[1]:


import json
import math
from collections import defaultdict
from tqdm import tqdm

# ì„¤ì •
INPUT_FILE = "dataset/review_5up_5aspect_3sentiment_vectorized_clean.json"
TRAIN_FILE = "dataset/train_80.json"
TEST_FILE = "dataset/test_20.json"
MIN_REVIEWS = 5

# ìœ ì €ë³„ ë¦¬ë·° ëª¨ìœ¼ê¸°
user_reviews = defaultdict(list)

with open(INPUT_FILE, encoding="utf-8") as f:
    for line in f:
        review = json.loads(line)
        user_reviews[review["user_id"]].append(review)

# ë¶„í• 
train_data, test_data = [], []

for uid, reviews in tqdm(user_reviews.items(), desc="20% ë¹„ìœ¨ë¡œ ë¶„í•  ì¤‘"):
    if len(reviews) < MIN_REVIEWS:
        continue

    # ì‹œê°„ìˆœ ì •ë ¬ ('date' í‚¤ê°€ ì—†ìœ¼ë©´ 'review_id' ë“±ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
    sorted_reviews = sorted(reviews, key=lambda x: x.get("date", x.get("review_id")))

    k = max(1, math.ceil(len(reviews) * 0.2))  # ìµœì†Œ 1ê°œëŠ” í‰ê°€ìš©
    train_data.extend(sorted_reviews[:-k])
    test_data.extend(sorted_reviews[-k:])

# ì €ì¥
with open(TRAIN_FILE, "w", encoding="utf-8") as f:
    for r in train_data:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

with open(TEST_FILE, "w", encoding="utf-8") as f:
    for r in test_data:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

print(f"âœ… í›ˆë ¨ ë°ì´í„° ê°œìˆ˜: {len(train_data)}")
print(f"âœ… í‰ê°€ ë°ì´í„° ê°œìˆ˜: {len(test_data)}")
print(f"ğŸ“Š ì‚¬ìš©ì ìˆ˜: {len(user_reviews)}")


# In[3]:


import json


def count_unique_users(file_path):
    user_set = set()
    with open(file_path, encoding="utf-8") as f:
        for line in f:
            review = json.loads(line)
            user_set.add(review["user_id"])
    return len(user_set)


train_file = "dataset/train_80.json"
test_file = "dataset/test_20.json"

train_user_count = count_unique_users(train_file)
test_user_count = count_unique_users(test_file)

print(f"ğŸ‘¥ í›ˆë ¨ ë°ì´í„°ì˜ ìœ ì € ìˆ˜: {train_user_count}")
print(f"ğŸ§ª í‰ê°€ ë°ì´í„°ì˜ ìœ ì € ìˆ˜: {test_user_count}")
