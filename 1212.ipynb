{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673869bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ë²¡í„° ìƒì„± ì¤‘: 452505it [2:17:08, 54.99it/s]\n",
      "ğŸ‘¤ ìœ ì € ë²¡í„° í‰ê· : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28483/28483 [00:13<00:00, 2082.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… user_vector.json ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ì„¤ì •\n",
    "ABSA_FILE = \"absa_ate_results.json\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "SENT2WEIGHT = {\"pos\": 1.0, \"neg\": -1.0}\n",
    "EMBED_DIM = 384  # ì„ë² ë”© ì°¨ì›\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# ìœ ì €ë³„ ëˆ„ì  ë²¡í„°\n",
    "user_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"ğŸ”„ ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "        review = json.loads(line)\n",
    "        user_id = review[\"user_id\"]\n",
    "        aspects = review.get(\"aspects\", [])\n",
    "\n",
    "        # ğŸ”¸ aspectsê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
    "        if not aspects:\n",
    "            continue\n",
    "\n",
    "        for asp in aspects:\n",
    "            term = asp[\"term\"].strip()\n",
    "            sentiment = asp[\"sentiment\"]\n",
    "            confidence = asp[\"confidence\"]\n",
    "\n",
    "            if sentiment not in SENT2WEIGHT:\n",
    "                continue\n",
    "\n",
    "            weight = SENT2WEIGHT[sentiment] * confidence\n",
    "            vec = model.encode(term)\n",
    "            user_vecs[user_id].append(weight * vec)\n",
    "\n",
    "# í‰ê·  ë²¡í„° ê³„ì‚°\n",
    "user_embed = {\n",
    "    uid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for uid, vecs in tqdm(user_vecs.items(), desc=\"ğŸ‘¤ ìœ ì € ë²¡í„° í‰ê· \")\n",
    "}\n",
    "\n",
    "# ì €ì¥\n",
    "with open(\"user_vector.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_embed, f, indent=2)\n",
    "\n",
    "print(\"âœ… user_vector.json ì €ì¥ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d756ff0",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc6b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82104\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Word2Vec ë¡œë”© ì¤‘...\n",
      "âœ… Word2Vec ë¡œë”© ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ ìœ ì € ë²¡í„° ìƒì„± ì¤‘: 452505it [00:07, 57646.92it/s]\n",
      "ğŸ‘¤ ìœ ì € ë²¡í„° í‰ê· : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8933/8933 [00:10<00:00, 879.80it/s] \n",
      "ğŸ  ì‹ë‹¹ ë²¡í„° ìƒì„± ì¤‘: 452505it [00:06, 66231.52it/s]\n",
      "ğŸ  ì‹ë‹¹ ë²¡í„° í‰ê· : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3325/3325 [00:00<00:00, 34719.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ì„¤ì •\n",
    "ABSA_FILE = \"absa_ate_results.json\"\n",
    "W2V_PATH = \"GoogleNews-vectors-negative300.bin.gz\"  # Word2Vec ëª¨ë¸\n",
    "EMBED_DIM = 300\n",
    "SENT2WEIGHT = {\"pos\": 1.0, \"neg\": -1.0}\n",
    "MIN_LEN = 2        # term ìµœì†Œ ë‹¨ì–´ ìˆ˜\n",
    "MIN_CONF = 0.7     # confidence threshold\n",
    "\n",
    "def is_valid_term(term, confidence, stop_words, min_len=2, min_conf=0.7):\n",
    "    tokens = term.strip().split()\n",
    "    if len(tokens) < min_len:\n",
    "        return False\n",
    "    if confidence < min_conf:\n",
    "        return False\n",
    "    if all(tok.lower() in stop_words for tok in tokens):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"ğŸ” Word2Vec ë¡œë”© ì¤‘...\")\n",
    "w2v_model = KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)\n",
    "print(\"âœ… Word2Vec ë¡œë”© ì™„ë£Œ!\")\n",
    "\n",
    "user_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"ğŸ‘¤ ìœ ì € ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "        obj = json.loads(line)\n",
    "        uid = obj[\"user_id\"]\n",
    "        for asp in obj.get(\"aspects\", []):\n",
    "            term = asp[\"term\"].strip()\n",
    "            sent = asp[\"sentiment\"]\n",
    "            conf = asp[\"confidence\"]\n",
    "\n",
    "            if sent not in SENT2WEIGHT:\n",
    "                continue\n",
    "            if not is_valid_term(term, conf, stop_words, MIN_LEN, MIN_CONF):\n",
    "                continue\n",
    "\n",
    "            tokens = term.split()\n",
    "            vecs = [w2v_model[tok] for tok in tokens if tok in w2v_model]\n",
    "            if not vecs:\n",
    "                continue\n",
    "\n",
    "            vec = np.mean(vecs, axis=0)\n",
    "            weight = SENT2WEIGHT[sent] * conf\n",
    "            user_vecs[uid].append(weight * vec)\n",
    "\n",
    "user_embed = {\n",
    "    uid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for uid, vecs in tqdm(user_vecs.items(), desc=\"ğŸ‘¤ ìœ ì € ë²¡í„° í‰ê· \")\n",
    "}\n",
    "\n",
    "with open(\"user_vector_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_embed, f, indent=2)\n",
    "\n",
    "biz_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"ğŸ  ì‹ë‹¹ ë²¡í„° ìƒì„± ì¤‘\"):\n",
    "        obj = json.loads(line)\n",
    "        bid = obj[\"business_id\"]\n",
    "        for asp in obj.get(\"aspects\", []):\n",
    "            term = asp[\"term\"].strip()\n",
    "            sent = asp[\"sentiment\"]\n",
    "            conf = asp[\"confidence\"]\n",
    "\n",
    "            if sent not in SENT2WEIGHT:\n",
    "                continue\n",
    "            if not is_valid_term(term, conf, stop_words, MIN_LEN, MIN_CONF):\n",
    "                continue\n",
    "\n",
    "            tokens = term.split()\n",
    "            vecs = [w2v_model[tok] for tok in tokens if tok in w2v_model]\n",
    "            if not vecs:\n",
    "                continue\n",
    "\n",
    "            vec = np.mean(vecs, axis=0)\n",
    "            weight = SENT2WEIGHT[sent] * conf\n",
    "            biz_vecs[bid].append(weight * vec)\n",
    "\n",
    "biz_embed = {\n",
    "    bid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for bid, vecs in tqdm(biz_vecs.items(), desc=\"ğŸ  ì‹ë‹¹ ë²¡í„° í‰ê· \")\n",
    "}\n",
    "\n",
    "with open(\"business_vector_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(biz_embed, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
