{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8327b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "file_path = \"data/yelp_academic_dataset_review.json\"\n",
    "\n",
    "# chunk ë‹¨ìœ„ë¡œ ì „ì²´ ë°ì´í„° ì½ê³  í•©ì¹˜ê¸°\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# ëª¨ë“  chunkë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì„œ ì—°ê²°\n",
    "df_r = pd.concat(chunk for chunk in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e925b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84461ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6990280 entries, 0 to 6990279\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   review_id    object        \n",
      " 1   user_id      object        \n",
      " 2   business_id  object        \n",
      " 3   stars        int64         \n",
      " 4   useful       int64         \n",
      " 5   funny        int64         \n",
      " 6   cool         int64         \n",
      " 7   text         object        \n",
      " 8   date         datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(4)\n",
      "memory usage: 480.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4092fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.strip().lower()                    # ì†Œë¬¸ìí™” + ì–‘ìª½ ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)            # URL ì œê±°\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", \" \", text)  # ì•ŒíŒŒë²³/ìˆ«ì/ê¸°ì´ˆ ë¬¸ì¥ë¶€í˜¸ ì™¸ ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # ê³µë°± ì •ë¦¬\n",
    "    return text\n",
    "\n",
    "# funnyì™€ cool ì»¬ëŸ¼ ì œê±°\n",
    "df = df.drop(columns=[\"funny\", \"cool\"])\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].astype(str).apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca032a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ” í† í° ê¸¸ì´ ê³„ì‚°: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6990280/6990280 [19:06<00:00, 6096.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í° í•„í„°ë§ ì™„ë£Œ: 2316286ê°œ ë¦¬ë·° ì œê±°ë¨ (ì´ 6990280 â†’ 4673994)\n",
      "â± ì†Œìš” ì‹œê°„: 19ë¶„ 37ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# 1. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# 2. í† í° ê¸¸ì´ ê³„ì‚° (ì§„í–‰ë¥  + ì‹œê°„ ì¸¡ì •)\n",
    "start = time.time()\n",
    "\n",
    "df[\"token_length\"] = [\n",
    "    len(tokenizer.tokenize(text)) for text in tqdm(df[\"cleaned_text\"], desc=\"ğŸ” í† í° ê¸¸ì´ ê³„ì‚°\", total=len(df))\n",
    "]\n",
    "\n",
    "# 3. í•„í„°ë§: í† í° ê¸¸ì´ 1~128 ì‚¬ì´ë§Œ ìœ ì§€\n",
    "before = len(df)\n",
    "df = df[(df[\"token_length\"] > 0) & (df[\"token_length\"] <= 128)]\n",
    "after = len(df)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"âœ… í† í° í•„í„°ë§ ì™„ë£Œ: {before - after}ê°œ ë¦¬ë·° ì œê±°ë¨ (ì´ {before} â†’ {after})\")\n",
    "print(f\"â± ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626e7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì „ì²´ ì»¬ëŸ¼ í¬í•¨ JSONL ì €ì¥ ì™„ë£Œ: data/review_0509.json\n",
      "âœ… ì €ì¥ëœ ë¦¬ë·° ìˆ˜: 4673994\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "output_path = \"data/review_0509.json\"\n",
    "\n",
    "# 1. datetime â†’ ISO ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "df[\"date\"] = df[\"date\"].astype(str)  # ë˜ëŠ” df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        json.dump(row, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"ğŸ“ ì „ì²´ ì»¬ëŸ¼ í¬í•¨ JSONL ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "print(f\"âœ… ì €ì¥ëœ ë¦¬ë·° ìˆ˜: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc822c6",
   "metadata": {},
   "source": [
    "# ê·¼ê±°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a00fd4",
   "metadata": {},
   "source": [
    "â‘  ë°ì´í„° Chunk ë‹¨ìœ„ ì²˜ë¦¬ ê·¼ê±°\n",
    "ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•  ë•Œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•œ Chunk ë‹¨ìœ„ ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ ë¬¸í—Œ:\n",
    "McKinney, W. (2011). pandas: a foundational Python library for data analysis and statistics. Python for High Performance and Scientific Computing, 14(9), 1-9.\n",
    "\n",
    "ğŸ”— ì¶œì²˜\n",
    "\n",
    "ì´ ë¬¸í—Œì€ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì  ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•ìœ¼ë¡œ chunk ê¸°ë°˜ ë°ì´í„° ë¡œë”©ì˜ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "\n",
    "â‘¡ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ê·¼ê±°\n",
    "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(íŠ¹íˆ NLP ëª¨ë¸)ì„ í›ˆë ¨í•˜ê¸° ì „ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ê³¼ì •ì˜ ì¤‘ìš”ì„±ì„ ì„¤ëª…í•˜ëŠ” ë¬¸í—Œì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ ë¬¸í—Œ:\n",
    "Haddi, E., Liu, X., & Shi, Y. (2013). The role of text pre-processing in sentiment analysis. Procedia Computer Science, 17, 26-32.\n",
    "\n",
    "ğŸ”— ì¶œì²˜ https://www.sciencedirect.com/science/article/pii/S1877050913001385\n",
    "\n",
    "ì´ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •ì´ ê°ì„± ë¶„ì„ ëª¨ë¸ì˜ ì •í™•ë„ì™€ ì„±ëŠ¥ì— ë§¤ìš° í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ê°•ì¡°í•©ë‹ˆë‹¤.\n",
    "\n",
    "â‘¢ í† í°í™” ë° ê¸¸ì´ ê¸°ë°˜ í•„í„°ë§ ê·¼ê±°\n",
    "ëª¨ë¸ í›ˆë ¨ ì‹œ ìµœëŒ€ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì°¸ê³ ë¬¸í—Œ:\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of NAACL 2019.\n",
    "\n",
    "ğŸ”— ì¶œì²˜ https://aclanthology.org/N19-1423/\n",
    "\n",
    "ì´ ë…¼ë¬¸ì€ Transformer ê¸°ë°˜ ëª¨ë¸ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •í•˜ê³  ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ë¥¼ ìë¥´ê±°ë‚˜ ì œê±°í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì„ì„ ì–¸ê¸‰í•˜ë©°, ì„±ëŠ¥ê³¼ í›ˆë ¨ ì†ë„ ê°œì„ ì— ì˜í–¥ì„ ì¤€ë‹¤ê³  ë³´ê³ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fd58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
