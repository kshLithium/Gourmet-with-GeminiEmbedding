{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87557c4a-85ab-43ea-821a-2c0a4f89297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 51962\n",
      "전체 데이터 수: 447796\n",
      "학습 데이터 수: 313456 (70.00%)\n",
      "검증 데이터 수: 44780 (10.00%)\n",
      "테스트 데이터 수: 89560 (20.00%)\n",
      "\n",
      "--- Starting training with fixed parameters ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.2}\n",
      "Epoch 1/50, Train Loss: 1.2615, Val Loss: 1.0294, Val RMSE: 1.0146\n",
      "  --> Validation RMSE improved. Model saved: 1.0146\n",
      "Epoch 2/50, Train Loss: 0.9329, Val Loss: 0.9273, Val RMSE: 0.9630\n",
      "  --> Validation RMSE improved. Model saved: 0.9630\n",
      "Epoch 3/50, Train Loss: 0.8216, Val Loss: 0.8727, Val RMSE: 0.9342\n",
      "  --> Validation RMSE improved. Model saved: 0.9342\n",
      "Epoch 4/50, Train Loss: 0.7385, Val Loss: 0.8407, Val RMSE: 0.9169\n",
      "  --> Validation RMSE improved. Model saved: 0.9169\n",
      "Epoch 5/50, Train Loss: 0.6735, Val Loss: 0.8232, Val RMSE: 0.9073\n",
      "  --> Validation RMSE improved. Model saved: 0.9073\n",
      "Epoch 6/50, Train Loss: 0.6159, Val Loss: 0.8526, Val RMSE: 0.9234\n",
      "Epoch 7/50, Train Loss: 0.5670, Val Loss: 0.8189, Val RMSE: 0.9050\n",
      "  --> Validation RMSE improved. Model saved: 0.9050\n",
      "Epoch 8/50, Train Loss: 0.5226, Val Loss: 0.8546, Val RMSE: 0.9245\n",
      "Epoch 9/50, Train Loss: 0.4824, Val Loss: 0.8917, Val RMSE: 0.9443\n",
      "Epoch 10/50, Train Loss: 0.4448, Val Loss: 0.8863, Val RMSE: 0.9414\n",
      "Epoch 11/50, Train Loss: 0.4092, Val Loss: 0.9330, Val RMSE: 0.9659\n",
      "Epoch 12/50, Train Loss: 0.3767, Val Loss: 0.9233, Val RMSE: 0.9609\n",
      "Epoch 13/50, Train Loss: 0.3479, Val Loss: 0.9364, Val RMSE: 0.9677\n",
      "Epoch 14/50, Train Loss: 0.3198, Val Loss: 0.9146, Val RMSE: 0.9564\n",
      "Epoch 15/50, Train Loss: 0.2976, Val Loss: 0.9492, Val RMSE: 0.9743\n",
      "Epoch 16/50, Train Loss: 0.2762, Val Loss: 1.0058, Val RMSE: 1.0029\n",
      "Epoch 17/50, Train Loss: 0.2583, Val Loss: 1.0250, Val RMSE: 1.0125\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "\n",
      "Loaded best model weights from best_afram_model.pt\n",
      "\n",
      "--- Final Model Performance on Test Set ---\n",
      "Used Hyperparameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.2}\n",
      "Mean Squared Error (MSE): 0.7952\n",
      "Root Mean Squared Error (RMSE): 0.8917\n",
      "Mean Absolute Error (MAE): 0.6711\n",
      "Mean Absolute Percentage Error (MAPE): 25.72%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# --- 유틸리티 함수 ---\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MAPE를 계산합니다. 0으로 나누는 오류를 방지합니다.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    if np.sum(non_zero_true) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "\n",
    "# --- 장치 설정 (GPU 사용 가능 시) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 데이터 전처리 및 어휘 구축 ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    텍스트를 소문자로 변환하고, 알파벳, 숫자, 공백만 남긴 후 단어 단위로 분리합니다 (토큰화).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    텍스트 데이터로부터 어휘를 구축하고, 단어를 정수 ID로 변환합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_freq):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1} # string_to_int: 패딩 토큰과 알 수 없는 단어 토큰 정의\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"} # int_to_string\n",
    "        self.freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocabulary(self, text_list):\n",
    "        \"\"\"\n",
    "        주어진 텍스트 리스트를 기반으로 어휘를 구축합니다.\n",
    "        min_freq보다 적게 나타나는 단어는 <UNK> 토큰으로 처리됩니다.\n",
    "        \"\"\"\n",
    "        for text in text_list:\n",
    "            self.freq.update(text)\n",
    "        \n",
    "        idx = 2 # <PAD>, <UNK> 다음 인덱스부터 시작\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"\n",
    "        텍스트(단어 리스트)를 정수 ID 시퀀스로 변환합니다.\n",
    "        \"\"\"\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "# --- 파일 로드 ---\n",
    "# 'review.json' 파일이 현재 스크립트와 동일한 디렉토리에 있어야 합니다.\n",
    "df = pd.read_json('review.json', lines=True)\n",
    "\n",
    "# --- 필요한 컬럼 추출 및 인코딩 ---\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'text']].copy()\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# --- 텍스트 전처리 및 어휘 구축 실행 ---\n",
    "all_texts = df_processed['text'].apply(preprocess_text).tolist()\n",
    "min_word_freq = 5 # 최소 단어 빈도수 설정 (조정 가능)\n",
    "vocab = Vocabulary(min_word_freq)\n",
    "vocab.build_vocabulary(all_texts)\n",
    "vocab_size = len(vocab.stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# 리뷰 텍스트를 정수 ID 시퀀스로 변환하고, 패딩/트렁케이션 적용\n",
    "MAX_REVIEW_LEN = 100 # 리뷰 텍스트의 최대 길이 (조정 가능)\n",
    "df_processed.loc[:, 'numericalized_text'] = df_processed['text'].apply(vocab.numericalize)\n",
    "df_processed['numericalized_text'] = df_processed['numericalized_text'].apply(\n",
    "    lambda x: x[:MAX_REVIEW_LEN] if len(x) > MAX_REVIEW_LEN else x + [vocab.stoi[\"<PAD>\"]] * (MAX_REVIEW_LEN - len(x))\n",
    ")\n",
    "\n",
    "# --- 데이터 분할 (7:1:2 비율) ---\n",
    "# 먼저 전체에서 테스트 세트 (20%) 분리\n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "# 남은 train_val_df (80%)에서 학습 세트 (70%)와 검증 세트 (10%) 분리\n",
    "# train_val_df는 전체의 80%이므로, 7:1 비율은 train_val_df의 7/8과 1/8이 됩니다.\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=42) \n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class AFRAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AFRAM 모델 학습을 위한 PyTorch Dataset 클래스.\n",
    "    사용자 ID, 사업체 ID, 수치화된 리뷰 텍스트, 평점을 반환합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.reviews = torch.tensor(np.array(df['numericalized_text'].tolist()), dtype=torch.long)\n",
    "        self.stars = torch.tensor(df['stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.reviews[idx], self.stars[idx]\n",
    "\n",
    "# --- Dataset 객체 생성 (DataLoader보다 먼저 정의되어야 함!) ---\n",
    "train_dataset = AFRAMDataset(train_df)\n",
    "val_dataset = AFRAMDataset(val_df)\n",
    "test_dataset = AFRAMDataset(test_df)\n",
    "\n",
    "# --- AFRAM 모델 아키텍처 정의 ---\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    리뷰 텍스트에서 CNN, LSTM, 어텐션 메커니즘을 사용하여 특징을 추출합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional Layer (CNN)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention Layer (Bahdanau-style Additive Attention)\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2) # Bi-LSTM의 출력 차원에 맞춤\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim * 2, 1)) # 어텐션 가중치 벡터 (학습 가능)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_seq):\n",
    "        # text_seq: (batch_size, seq_len)\n",
    "        embedded = self.embedding(text_seq) # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = embedded.permute(0, 2, 1) # Conv1d를 위해 차원 변경 (batch_size, embedding_dim, seq_len)\n",
    "        \n",
    "        conv_out = torch.relu(self.conv(embedded)) # (batch_size, hidden_dim, seq_len)\n",
    "        conv_out = conv_out.permute(0, 2, 1) # LSTM을 위해 차원 변경 (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        lstm_out, _ = self.lstm(self.dropout(conv_out)) # (batch_size, seq_len, hidden_dim * 2) (Bi-LSTM)\n",
    "        \n",
    "        # 어텐션 메커니즘 적용\n",
    "        attn_weights = torch.tanh(self.attn_proj(lstm_out)) # (batch_size, seq_len, hidden_dim * 2)\n",
    "        v_expanded = self.v.unsqueeze(0).expand(attn_weights.shape[0], -1, -1) # 배치 크기에 맞게 v 확장\n",
    "        \n",
    "        scores = torch.bmm(attn_weights, v_expanded) # 어텐션 스코어 계산 (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(scores, dim=1) # 소프트맥스로 가중치 정규화 (sum=1)\n",
    "        \n",
    "        # 문맥 벡터 (가중합) 계산: 어텐션 가중치를 적용한 LSTM 출력의 가중 평균\n",
    "        context_vector = torch.sum(lstm_out * attention_weights, dim=1) # (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        return context_vector # 이 벡터가 리뷰의 \"측면 특징\"을 나타냅니다.\n",
    "\n",
    "class AFRAMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    AFRAM 논문의 전체 모델 구조를 구현합니다.\n",
    "    사용자-사업체 상호작용과 리뷰 텍스트 특징을 결합하여 평점을 예측합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                 text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate):\n",
    "        super(AFRAMModel, self).__init__()\n",
    "        \n",
    "        # 사용자 및 사업체 임베딩 레이어\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        \n",
    "        # 리뷰 텍스트를 인코딩하는 모듈 (어텐션 포함)\n",
    "        self.review_encoder = TextEncoderWithAttention(vocab_size, embedding_dim, text_encoder_hidden_dim, dropout_rate)\n",
    "\n",
    "        # 사용자-사업체 상호작용 MLP (논문의 Customer-Restaurant Interaction Module)\n",
    "        user_item_mlp_input_dim = embedding_dim * 2\n",
    "        user_item_layers = []\n",
    "        for dim in user_item_mlp_dims:\n",
    "            user_item_layers.append(nn.Linear(user_item_mlp_input_dim, dim))\n",
    "            user_item_layers.append(nn.ReLU())\n",
    "            user_item_mlp_input_dim = dim\n",
    "        self.user_item_mlp = nn.Sequential(*user_item_layers)\n",
    "        self.user_item_mlp_output_dim = user_item_mlp_dims[-1] if user_item_mlp_dims else embedding_dim * 2\n",
    "\n",
    "        # 최종 평점 예측 MLP (논문의 Rating Prediction Module)\n",
    "        final_mlp_input_dim = self.user_item_mlp_output_dim + \\\n",
    "                              text_encoder_hidden_dim * 2 # review_encoder의 출력 차원 (Bi-LSTM이므로 hidden_dim * 2)\n",
    "        \n",
    "        final_layers = []\n",
    "        for dim in final_mlp_dims:\n",
    "            final_layers.append(nn.Linear(final_mlp_input_dim, dim))\n",
    "            final_layers.append(nn.ReLU())\n",
    "            final_mlp_input_dim = dim\n",
    "        final_layers.append(nn.Linear(final_mlp_input_dim, 1)) # 최종 출력은 평점 (1차원)\n",
    "        self.prediction_mlp = nn.Sequential(*final_layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, reviews):\n",
    "        # 사용자 및 사업체 임베딩 벡터 가져오기\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        \n",
    "        # 사용자-사업체 임베딩을 결합하고 MLP에 통과시켜 상호작용 특징 생성\n",
    "        user_item_combined = torch.cat((user_vec, business_vec), dim=1)\n",
    "        user_item_features = self.user_item_mlp(user_item_combined)\n",
    "\n",
    "        # 리뷰 텍스트를 리뷰 인코더에 통과시켜 텍스트 특징(측면 특징) 추출\n",
    "        review_features = self.review_encoder(reviews)\n",
    "        \n",
    "        # 상호작용 특징과 리뷰 텍스트 특징을 결합\n",
    "        combined_features = torch.cat((user_item_features, review_features), dim=1)\n",
    "        \n",
    "        # 최종 평점 예측 MLP에 통과시켜 결과 반환\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.squeeze() # 1차원 평점 반환을 위해 차원 축소\n",
    "\n",
    "# --- 모델 학습 및 평가 (단일 파라미터 세트) ---\n",
    "# 최적 또는 기본적으로 사용할 하이퍼파라미터 설정\n",
    "# 이 값들은 논문이나 일반적인 딥러닝 모델에서 좋은 성능을 보이는 값들입니다.\n",
    "# 필요에 따라 이 값을 직접 변경하여 실험할 수 있습니다.\n",
    "params = {\n",
    "    'embedding_dim': 64,\n",
    "    'text_encoder_hidden_dim': 128,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'user_item_mlp_dims': [128, 64],\n",
    "    'final_mlp_dims': [64, 32],\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Starting training with fixed parameters ---\")\n",
    "print(f\"Parameters: {params}\")\n",
    "\n",
    "# 파라미터 언팩\n",
    "embedding_dim = params['embedding_dim']\n",
    "text_encoder_hidden_dim = params['text_encoder_hidden_dim']\n",
    "learning_rate = params['learning_rate']\n",
    "batch_size = params['batch_size']\n",
    "user_item_mlp_dims = params['user_item_mlp_dims']\n",
    "final_mlp_dims = params['final_mlp_dims']\n",
    "dropout_rate = params['dropout_rate']\n",
    "\n",
    "epochs = 50 # 최대 에폭 수\n",
    "patience = 10 # 조기 종료를 위한 검증 성능 개선 대기 에폭 수\n",
    "min_delta = 0.0005 # 성능 개선으로 인정할 최소 변화량\n",
    "\n",
    "best_val_rmse = float('inf')\n",
    "epochs_no_improve = 0\n",
    "model_save_path = 'best_afram_model.pt' # 최적 모델 저장 경로\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 인스턴스 생성 및 GPU로 이동\n",
    "model = AFRAMModel(num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                   text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate).to(device)\n",
    "\n",
    "criterion = nn.MSELoss() # 손실 함수: MSE\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # 옵티마이저: Adam\n",
    "\n",
    "# --- 학습 루프 (조기 종료 포함) ---\n",
    "for epoch in range(epochs):\n",
    "    model.train() # 모델을 학습 모드로 설정\n",
    "    total_train_loss = 0\n",
    "    for user_ids, business_ids, reviews, stars in train_loader:\n",
    "        # 데이터를 GPU로 이동\n",
    "        user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # 옵티마이저의 기울기 초기화\n",
    "        predictions = model(user_ids, business_ids, reviews) # 예측 수행\n",
    "        loss = criterion(predictions, stars) # 손실 계산\n",
    "        loss.backward() # 역전파\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_ratings = []\n",
    "    with torch.no_grad(): # 기울기 계산 비활성화 (메모리 절약, 속도 향상)\n",
    "        for user_ids, business_ids, reviews, stars in val_loader:\n",
    "            # 데이터를 GPU로 이동\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            \n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            loss = criterion(predictions, stars)\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_true_ratings.extend(stars.tolist())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {total_val_loss / len(val_loader):.4f}, Val RMSE: {current_val_rmse:.4f}\")\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    # 현재 검증 RMSE가 이전까지의 최고 검증 RMSE보다 min_delta 이상 개선되었다면\n",
    "    if current_val_rmse < best_val_rmse - min_delta:\n",
    "        best_val_rmse = current_val_rmse\n",
    "        epochs_no_improve = 0 # 개선되었으니 대기 카운트 초기화\n",
    "        torch.save(model.state_dict(), model_save_path) # 최적 모델 저장\n",
    "        print(f\"  --> Validation RMSE improved. Model saved: {best_val_rmse:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1 # 개선되지 않았으니 대기 카운트 증가\n",
    "        if epochs_no_improve == patience: # 대기 카운트가 patience에 도달하면\n",
    "            print(f\"  Early stopping! No improvement in RMSE for {patience} epochs.\")\n",
    "            break # 학습 중단\n",
    "\n",
    "# --- 최종 모델 테스트 ---\n",
    "if os.path.exists(model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(f\"\\nLoaded best model weights from {model_save_path}\")\n",
    "else:\n",
    "    print(f\"\\nCould not find best model weights at '{model_save_path}'. Testing with current model state.\")\n",
    "\n",
    "model.eval() # 모델을 평가 모드로 설정\n",
    "test_predictions = []\n",
    "true_ratings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_ids, business_ids, reviews, stars in test_loader:\n",
    "        user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "        predictions = model(user_ids, business_ids, reviews)\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "        true_ratings.extend(stars.tolist())\n",
    "\n",
    "mse = mean_squared_error(true_ratings, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "mape = mean_absolute_percentage_error(true_ratings, test_predictions)\n",
    "\n",
    "print(f\"\\n--- Final Model Performance on Test Set ---\")\n",
    "print(f\"Used Hyperparameters: {params}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba48203-7794-4cce-9323-28f4616f0e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
