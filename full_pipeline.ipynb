{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data Preparation Pipeline\n",
    "This notebook consolidates all preprocessing steps used to create `review_business_5up_5aspect_3sentiment_vectorized_clean.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 6855ê°œ í•­ëª©ì´ 'data/output/business.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: business.json preprocessing ---\n",
    "df_B = pd.read_json(\"data/raw/yelp_academic_dataset_business.json\", lines=True)\n",
    "business_df = df_B.copy()\n",
    "drop_cols = ['postal_code','latitude','longitude','attributes','hours']\n",
    "business_df = business_df.drop(columns=drop_cols)\n",
    "business_df.loc[business_df['city'].str.lower().str.contains(\"philadelphia\", na=False),'city'] = \"Philadelphia\"\n",
    "\n",
    "def load_categories(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "food_categories = load_categories('data/raw/food.txt')\n",
    "restaurant_categories = load_categories('data/raw/restaurant.txt')\n",
    "target_categories = food_categories.union(restaurant_categories)\n",
    "\n",
    "def category_match(row):\n",
    "    if isinstance(row,str):\n",
    "        biz_categories = set(cat.strip().lower() for cat in row.split(','))\n",
    "        return bool(biz_categories & target_categories)\n",
    "    return False\n",
    "business_food_df = business_df[business_df['categories'].apply(category_match)]\n",
    "top_state = business_food_df['state'].value_counts().idxmax()\n",
    "business_pa_df = business_food_df[business_food_df['state']==top_state]\n",
    "business_paph_df = business_pa_df[business_pa_df['city']==\"Philadelphia\"]\n",
    "mask = business_paph_df.apply(lambda col: col.map(lambda x: pd.isna(x) or (isinstance(x,str) and x.strip()==\"\"))).any(axis=1)\n",
    "business_paph_df_2 = business_paph_df[~mask].reset_index(drop=True)\n",
    "business_paph_df_2.to_json(\"data/output/business.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "print(f\"ì´ {len(business_paph_df_2)}ê°œ í•­ëª©ì´ 'data/output/business.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 730552/730552 [02:11<00:00, 5541.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: review.json preprocessing ---\n",
    "chunk_size=100000\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)\n",
    "df_review=pd.concat(chunk for chunk in chunks)\n",
    "business_ids=set(business_paph_df_2['business_id'])\n",
    "df_review=df_review[df_review['business_id'].isin(business_ids)]\n",
    "df_review=df_review.drop(columns=['funny','cool'])\n",
    "\n",
    "tqdm.pandas()\n",
    "tokenizer=DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "df_review['token_length']=df_review['text'].progress_apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_review.to_json(\"data/output/review.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: user.json preprocessing ---\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_user.json\", lines=True, chunksize=100000)\n",
    "df_user=pd.concat(chunk for chunk in chunks)\n",
    "drop_columns=['yelping_since','funny','cool','elite','friends','fans','compliment_hot','compliment_more','compliment_profile','compliment_cute','compliment_list','compliment_note','compliment_plain','compliment_cool','compliment_funny','compliment_writer','compliment_photos']\n",
    "df_user=df_user.drop(columns=drop_columns)\n",
    "review_counts=df_review['user_id'].value_counts()\n",
    "user_ids_5plus=review_counts[review_counts>=5].index\n",
    "df_user=df_user[df_user['user_id'].isin(user_ids_5plus)]\n",
    "df_user.to_json(\"data/output/user.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë³‘í•© ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 730552/730552 [00:02<00:00, 281974.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³‘í•© ì™„ë£Œ: merged_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: merge review, user and business ---\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "reviews = load_jsonl(\"data/output/review.json\")\n",
    "users = load_jsonl(\"data/output/user.json\")\n",
    "businesses = load_jsonl(\"data/output/business.json\")\n",
    "\n",
    "user_dict = {u['user_id']: u for u in users}\n",
    "business_dict = {b['business_id']: b for b in businesses}\n",
    "\n",
    "merged_data = []\n",
    "for r in tqdm(reviews, desc='ë³‘í•© ì¤‘'):\n",
    "    uid = r['user_id']\n",
    "    bid = r['business_id']\n",
    "    if uid in user_dict and bid in business_dict:\n",
    "        m = r.copy()\n",
    "        for k, v in user_dict[uid].items():\n",
    "            m[f'user_{k}'] = v\n",
    "        for k, v in business_dict[bid].items():\n",
    "            m[f'business_{k}'] = v\n",
    "        merged_data.append(m)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame(merged_data)\n",
    "# ë¶ˆí•„ìš”í•œ ID ì»¬ëŸ¼ ì œê±°\n",
    "df.drop(columns=[\"user_user_id\", \"business_business_id\"], inplace=True, errors=\"ignore\")\n",
    "# ì»¬ëŸ¼ ì´ë¦„ ë¦¬ë„¤ì´ë°\n",
    "df.rename(columns={\n",
    "    \"stars\": \"review_stars\",\n",
    "    \"useful\": \"review_useful\",\n",
    "    \"date\": \"review_date\"}, inplace=True)\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "output_path = \"data/output/merged_dataset.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "# JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        json.dump(row, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"ë³‘í•© ì™„ë£Œ: merged_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a2d956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š [ê¸°ë³¸ ì •ë³´]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 451185 entries, 0 to 451184\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   review_id              451185 non-null  object \n",
      " 1   user_id                451185 non-null  object \n",
      " 2   business_id            451185 non-null  object \n",
      " 3   review_stars           451185 non-null  int64  \n",
      " 4   review_useful          451185 non-null  int64  \n",
      " 5   text                   451185 non-null  object \n",
      " 6   review_date            451185 non-null  int64  \n",
      " 7   token_length           451185 non-null  int64  \n",
      " 8   user_name              451185 non-null  object \n",
      " 9   user_review_count      451185 non-null  int64  \n",
      " 10  user_useful            451185 non-null  int64  \n",
      " 11  user_average_stars     451185 non-null  float64\n",
      " 12  business_name          451185 non-null  object \n",
      " 13  business_address       451185 non-null  object \n",
      " 14  business_city          451185 non-null  object \n",
      " 15  business_state         451185 non-null  object \n",
      " 16  business_stars         451185 non-null  float64\n",
      " 17  business_review_count  451185 non-null  int64  \n",
      " 18  business_is_open       451185 non-null  int64  \n",
      " 19  business_categories    451185 non-null  object \n",
      "dtypes: float64(2), int64(8), object(10)\n",
      "memory usage: 68.8+ MB\n",
      "None\n",
      "\n",
      "ğŸ§¾ [ì»¬ëŸ¼ ëª©ë¡]\n",
      "['review_id', 'user_id', 'business_id', 'review_stars', 'review_useful', 'text', 'review_date', 'token_length', 'user_name', 'user_review_count', 'user_useful', 'user_average_stars', 'business_name', 'business_address', 'business_city', 'business_state', 'business_stars', 'business_review_count', 'business_is_open', 'business_categories']\n",
      "\n",
      "ğŸ” [ìƒ˜í”Œ ë°ì´í„°]\n",
      "             review_id                user_id            business_id  review_stars  review_useful                                                                                                                                                                                                                                                                                                                                                                                                                                text   review_date  token_length user_name  user_review_count  user_useful  user_average_stars     business_name business_address business_city business_state  business_stars  business_review_count  business_is_open                    business_categories\n",
      "8JFGBuHMoiNDyfcxuWNtrA smOvOajNG0lS4Pq7d8g4JQ RZtGWDLCAtuipwaZ-UfjmQ             4              0                                                                                                                                                                                                                                                  Good food--loved the gnocchi with marinara\\nthe baked eggplant appetizer was very good too\\n\\nThe service was very slow, but despite this, I'd go back, the food is just that good 1255550234000            39       Meg                176          120                3.46         LaScala's  615 Chestnut St  Philadelphia             PA             3.5                    367                 0     Pizza, Restaurants, Italian, Salad\n",
      "Xs8Z8lmKkosqW5mw_sVAoA IQsF3Rc6IgCzjVV9DE8KXg eFvzHawVJofxSnD7TgbZtg             5              0 My absolute favorite cafe in the city. Their black and white latte is probably the best I've ever had (not too sweet and just the right amount of foam), soups are always really good (even for non-soup people) and there's just a lot of space to do work. The noise level is perfect, the music is at a perfect level, and I always enjoy when patrons bring their dogs. I'll keep giving them my business for as long as I can. 1415806227000           100      Dana                182          217                3.41   Good Karma Cafe      928 Pine St  Philadelphia             PA             4.0                    249                 1 Food, Cafes, Coffee & Tea, Restaurants\n",
      "JBWZmBy69VMggxj3eYn17Q aFa96pz67TwOFu4Weq5Agg kq5Ghhh14r-eCxlVmlyd8w             5              0                           My boyfriend and I tried this deli for the first time today. I had a turkey, avocado & bacon panini and he ha a buffalo chicken wrap. We will definitely be returning. The wait for food wasn't too long, which is always appreciated during lunch hour. There was SO much to choose from. They have salads, soup, macaroni, sandwiches and hot food. I love a deli that has many options to choose from! 1535060378000            87     Jenna                 84           75                3.93 The Coventry Deli   2000 Market St  Philadelphia             PA             4.0                     65                 1  Restaurants, Delis, Salad, Sandwiches\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 1. JSONL íŒŒì¼ ê²½ë¡œ\n",
    "path = \"data/output/merged_dataset.json\"  # í•„ìš” ì‹œ ê²½ë¡œ ìˆ˜ì •\n",
    "\n",
    "# 2. JSONL íŒŒì¼ ì½ê¸°\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# 3. pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 4. ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "print(\"ğŸ“Š [ê¸°ë³¸ ì •ë³´]\")\n",
    "print(df.info())\n",
    "print()\n",
    "\n",
    "# 5. ì»¬ëŸ¼ ëª©ë¡\n",
    "print(\"ğŸ§¾ [ì»¬ëŸ¼ ëª©ë¡]\")\n",
    "print(df.columns.tolist())\n",
    "print()\n",
    "\n",
    "# 6. ì˜ˆì‹œ ë°ì´í„°\n",
    "print(\"ğŸ” [ìƒ˜í”Œ ë°ì´í„°]\")\n",
    "print(df.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: attach IDs to ABSA results and vectorize ---\n",
    "# filter merged dataset for users with >=5 reviews\n",
    "user_review_counts=defaultdict(int)\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_review_counts[obj['user_id']]+=1\n",
    "qualified_users={u for u,c in user_review_counts.items() if c>=5}\n",
    "filtered_reviews=[]\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        if obj['user_id'] in qualified_users:\n",
    "            filtered_reviews.append(obj)\n",
    "with open(\"merged_dataset_5up_users_only.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in filtered_reviews:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"âœ… í•„í„°ë§ ì™„ë£Œ:\",len(filtered_reviews),'ê°œ ë¦¬ë·° ì €ì¥ â†’ merged_dataset_5up_users_only.json')\n",
    "\n",
    "id_map={}\n",
    "for obj in filtered_reviews:\n",
    "    rid=obj['review_id']\n",
    "    id_map[rid]={\n",
    "        'user_id':obj['user_id'],\n",
    "        'business_id':obj['business_id'],\n",
    "        'stars':obj['review_stars'],\n",
    "        'review_useful':obj['review_useful'],\n",
    "        'review_date':obj['review_date']\n",
    "    }\n",
    "updated=[]\n",
    "with open(\"review_5up_5aspect_3sentiment.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='ID ë° í‰ì  ì¶”ê°€ ì¤‘'):\n",
    "        obj=json.loads(line)\n",
    "        rid=obj.get('review_id')\n",
    "        if rid in id_map:\n",
    "            obj.update(id_map[rid])\n",
    "            updated.append(obj)\n",
    "with open(\"review_5up_5aspect_3sentiment_with_ids.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in updated:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\",len(updated),'ê±´ â†’ review_5up_5aspect_3sentiment_with_ids.json')\n",
    "\n",
    "input_file=\"review_5up_5aspect_3sentiment_with_ids.json\"\n",
    "output_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "\n",
    "def sentiment_to_vector(sentiment_dict):\n",
    "    aspects=['food','service','price','ambience','location']\n",
    "    polarities=['Negative','Neutral','Positive']\n",
    "    vector=[]\n",
    "    for asp in aspects:\n",
    "        scores=sentiment_dict.get(asp,{}).get('scores',{})\n",
    "        for pol in polarities:\n",
    "            vector.append(scores.get(pol,0.0))\n",
    "    return vector\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        vec=sentiment_to_vector(obj.get('sentiment',{}))\n",
    "        cleaned={'review_id':obj.get('review_id'),'user_id':obj.get('user_id'),'business_id':obj.get('business_id'),'stars':obj.get('stars'),'review_date':obj.get('review_date'),'sentiment_vector':vec}\n",
    "        fout.write(json.dumps(cleaned,ensure_ascii=False)+\"\")\n",
    "print(\"ì™„ë£Œ: textì™€ sentiment ì œê±° í›„ ì €ì¥ â†’\",output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: filter users with <5 unique businesses ---\n",
    "input_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "output_file=\"review_business_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "user_biz_ids=defaultdict(set)\n",
    "with open(input_file,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_biz_ids[obj['user_id']].add(obj['business_id'])\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        if len(user_biz_ids[obj['user_id']])>=5:\n",
    "            fout.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"ì™„ë£Œ: business_idê°€ 5ê°œ ë¯¸ë§Œì¸ ì‚¬ìš©ì ì œê±° í›„ ì €ì¥ â†’\",output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
